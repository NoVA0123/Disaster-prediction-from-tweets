{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/nlp-getting-started'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-24T08:54:45.628320Z","iopub.execute_input":"2023-09-24T08:54:45.628720Z","iopub.status.idle":"2023-09-24T08:54:45.637085Z","shell.execute_reply.started":"2023-09-24T08:54:45.628688Z","shell.execute_reply":"2023-09-24T08:54:45.635931Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I am going to use tensorflow library for Natural Language Processing","metadata":{}},{"cell_type":"code","source":"# Importing libraries\n\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:45.639050Z","iopub.execute_input":"2023-09-24T08:54:45.639684Z","iopub.status.idle":"2023-09-24T08:54:45.651281Z","shell.execute_reply.started":"2023-09-24T08:54:45.639652Z","shell.execute_reply":"2023-09-24T08:54:45.650218Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Importing the data using pandas","metadata":{}},{"cell_type":"code","source":"# Storing the data \n\ntraindata = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntestdata = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:45.654762Z","iopub.execute_input":"2023-09-24T08:54:45.655047Z","iopub.status.idle":"2023-09-24T08:54:45.695294Z","shell.execute_reply.started":"2023-09-24T08:54:45.655024Z","shell.execute_reply":"2023-09-24T08:54:45.694406Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Storing train data\n\ntrain_data_list = traindata['text'].tolist()\ntrain_labels = np.array(traindata['target'].tolist())\n\n# Storing test data\n\nevaluate_data_list = testdata['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:45.696440Z","iopub.execute_input":"2023-09-24T08:54:45.697228Z","iopub.status.idle":"2023-09-24T08:54:45.705310Z","shell.execute_reply.started":"2023-09-24T08:54:45.697192Z","shell.execute_reply":"2023-09-24T08:54:45.704162Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"We need to convert english words into array. Tokenizer will help me, as it has function which will give me the vocab size.","metadata":{}},{"cell_type":"code","source":"# creating a function to store in suitable form\n\ndef nlp_store(data):\n\n    token = tf.keras.preprocessing.text.Tokenizer()\n    token.fit_on_texts(data)\n\n    # storing the vocab size for padding\n    vocab_size = len(token.word_index) + 1\n    train_data = token.texts_to_sequences(data)\n    train_data = np.array(train_data)\n    \n    return train_data, vocab_size","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:45.708109Z","iopub.execute_input":"2023-09-24T08:54:45.708475Z","iopub.status.idle":"2023-09-24T08:54:45.716436Z","shell.execute_reply.started":"2023-09-24T08:54:45.708436Z","shell.execute_reply":"2023-09-24T08:54:45.715528Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Storing in suitable form\n\ntrain_data, vocab_size = nlp_store(train_data_list)\nevaluate_data, vocab_none = nlp_store(evaluate_data_list)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:45.717796Z","iopub.execute_input":"2023-09-24T08:54:45.718679Z","iopub.status.idle":"2023-09-24T08:54:46.250282Z","shell.execute_reply.started":"2023-09-24T08:54:45.718646Z","shell.execute_reply":"2023-09-24T08:54:46.248468Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_2354/2644733524.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  train_data = np.array(train_data)\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_data","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:46.251707Z","iopub.execute_input":"2023-09-24T08:54:46.252351Z","iopub.status.idle":"2023-09-24T08:54:46.259971Z","shell.execute_reply.started":"2023-09-24T08:54:46.252315Z","shell.execute_reply":"2023-09-24T08:54:46.258824Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"array([list([34, 567, 5, 985, 135, 100]),\n       list([272, 53, 247, 11, 1907, 1103, 743, 890, 429]),\n       list([75, 11, 5, 130, 42, 22, 891, 1908, 4022, 23, 4023, 986, 4, 568, 10, 1276, 481, 101, 43]),\n       ..., list([714, 299, 732, 7, 914, 3, 1, 2, 12817]),\n       list([3758, 697, 469, 379, 2469, 2470, 3, 1, 2, 12818]),\n       list([3412, 58, 3419, 277, 2396, 71, 264, 2464])], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:46.261440Z","iopub.execute_input":"2023-09-24T08:54:46.261897Z","iopub.status.idle":"2023-09-24T08:54:46.273166Z","shell.execute_reply.started":"2023-09-24T08:54:46.261862Z","shell.execute_reply":"2023-09-24T08:54:46.272182Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"22701"},"metadata":{}}]},{"cell_type":"markdown","source":"Each tweet contains maximum 4000 characters. So, I need to create a padding sequence which will store only 100 words","metadata":{}},{"cell_type":"code","source":"# padding\n\ntrain_data = tf.keras.utils.pad_sequences(train_data,\n                                          100)\nevaluate_data = tf.keras.utils.pad_sequences(evaluate_data,\n                                        100)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:46.275612Z","iopub.execute_input":"2023-09-24T08:54:46.276235Z","iopub.status.idle":"2023-09-24T08:54:46.341963Z","shell.execute_reply.started":"2023-09-24T08:54:46.276202Z","shell.execute_reply":"2023-09-24T08:54:46.341096Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"To build my model, I will use Sequential, Long Short Term Memory(LSTM), Embeddings, and final dense layer for output.","metadata":{}},{"cell_type":"code","source":"# creating model\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 32),\n    tf.keras.layers.LSTM(32),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:46.345281Z","iopub.execute_input":"2023-09-24T08:54:46.345621Z","iopub.status.idle":"2023-09-24T08:54:46.611973Z","shell.execute_reply.started":"2023-09-24T08:54:46.345576Z","shell.execute_reply":"2023-09-24T08:54:46.611015Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:46.613247Z","iopub.execute_input":"2023-09-24T08:54:46.614140Z","iopub.status.idle":"2023-09-24T08:54:46.638769Z","shell.execute_reply.started":"2023-09-24T08:54:46.614104Z","shell.execute_reply":"2023-09-24T08:54:46.638002Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_2 (Embedding)     (None, None, 32)          726432    \n                                                                 \n lstm_2 (LSTM)               (None, 32)                8320      \n                                                                 \n dense_3 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 734,785\nTrainable params: 734,785\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Compiling the model and fitting the data","metadata":{}},{"cell_type":"code","source":"model.compile(loss = 'BinaryCrossentropy',\n             optimizer = 'adam',\n             metrics = ['acc'])\n\nhistory = model.fit(train_data,\n                   train_labels,\n                   epochs = 20,\n                   validation_split = 0.1)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:54:46.639708Z","iopub.execute_input":"2023-09-24T08:54:46.640033Z","iopub.status.idle":"2023-09-24T08:56:10.347932Z","shell.execute_reply.started":"2023-09-24T08:54:46.640001Z","shell.execute_reply":"2023-09-24T08:56:10.346795Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/20\n215/215 [==============================] - 19s 76ms/step - loss: 0.5578 - acc: 0.7095 - val_loss: 0.4523 - val_acc: 0.7913\nEpoch 2/20\n215/215 [==============================] - 4s 21ms/step - loss: 0.3149 - acc: 0.8739 - val_loss: 0.4642 - val_acc: 0.8005\nEpoch 3/20\n215/215 [==============================] - 3s 14ms/step - loss: 0.1790 - acc: 0.9368 - val_loss: 0.5697 - val_acc: 0.7638\nEpoch 4/20\n215/215 [==============================] - 3s 13ms/step - loss: 0.1018 - acc: 0.9656 - val_loss: 0.5496 - val_acc: 0.7717\nEpoch 5/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0621 - acc: 0.9806 - val_loss: 0.6802 - val_acc: 0.7730\nEpoch 6/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0406 - acc: 0.9886 - val_loss: 0.7345 - val_acc: 0.7638\nEpoch 7/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0310 - acc: 0.9915 - val_loss: 0.6894 - val_acc: 0.7795\nEpoch 8/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0205 - acc: 0.9933 - val_loss: 0.9604 - val_acc: 0.7795\nEpoch 9/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0211 - acc: 0.9931 - val_loss: 0.8437 - val_acc: 0.7598\nEpoch 10/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0178 - acc: 0.9927 - val_loss: 0.9363 - val_acc: 0.7598\nEpoch 11/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0138 - acc: 0.9942 - val_loss: 0.9340 - val_acc: 0.7690\nEpoch 12/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0117 - acc: 0.9946 - val_loss: 1.0429 - val_acc: 0.7612\nEpoch 13/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0109 - acc: 0.9939 - val_loss: 0.9774 - val_acc: 0.7651\nEpoch 14/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0122 - acc: 0.9946 - val_loss: 0.9357 - val_acc: 0.7782\nEpoch 15/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0106 - acc: 0.9945 - val_loss: 0.9692 - val_acc: 0.7625\nEpoch 16/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0121 - acc: 0.9945 - val_loss: 0.8801 - val_acc: 0.7480\nEpoch 17/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0128 - acc: 0.9940 - val_loss: 1.0197 - val_acc: 0.7507\nEpoch 18/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0093 - acc: 0.9946 - val_loss: 1.2938 - val_acc: 0.7480\nEpoch 19/20\n215/215 [==============================] - 2s 11ms/step - loss: 0.0081 - acc: 0.9952 - val_loss: 1.3421 - val_acc: 0.7612\nEpoch 20/20\n215/215 [==============================] - 2s 9ms/step - loss: 0.0083 - acc: 0.9952 - val_loss: 1.3329 - val_acc: 0.7612\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Predicting the value and storing the value in either 1 or 0","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(evaluate_data)\n\npred = np.where(prediction > 0.5, 1, 0)\npred","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:56:10.349824Z","iopub.execute_input":"2023-09-24T08:56:10.350743Z","iopub.status.idle":"2023-09-24T08:56:11.407113Z","shell.execute_reply.started":"2023-09-24T08:56:10.350704Z","shell.execute_reply":"2023-09-24T08:56:11.405997Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"102/102 [==============================] - 1s 3ms/step\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"array([[1],\n       [0],\n       [1],\n       ...,\n       [0],\n       [1],\n       [0]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Exporting the testdata values","metadata":{}},{"cell_type":"code","source":"testdata[\"target\"] = pred\nsubmission = testdata.drop(['keyword', 'location', 'text'], axis = 1)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:56:11.408817Z","iopub.execute_input":"2023-09-24T08:56:11.409255Z","iopub.status.idle":"2023-09-24T08:56:11.426885Z","shell.execute_reply.started":"2023-09-24T08:56:11.409217Z","shell.execute_reply":"2023-09-24T08:56:11.425824Z"},"trusted":true},"execution_count":34,"outputs":[]}]}